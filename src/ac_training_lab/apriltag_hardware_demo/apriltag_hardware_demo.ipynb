{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4540c20",
   "metadata": {},
   "source": [
    "# AprilTag Minimal Working Example with Dedicated Hardware \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AccelerationConsortium/ac-training-lab/blob/main/src/ac_training_lab/apriltag_hardware_demo/apriltag_hardware_demo.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13dbd57",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "### What this system does\n",
    "\n",
    "This system uses a rotating turntable, AprilTag visual markers, and a MyCobot robotic arm to identify, localize, and manipulate vials. The robot can automatically pick up or place vials at specified positions on the turntable.\n",
    "\n",
    "### Why AprilTags are used\n",
    "\n",
    "AprilTags provide robust and accurate visual markers that enable the system to detect specific positions on the turntable. Since tags are attached to the turntable (not the vials), they help determine the vial’s location indirectly through a mapping. This allows the system to precisely stop the turntable at the right spot for pick-and-place operations.\n",
    "\n",
    "### How the workflow proceeds in steps\n",
    "1. After startup, the turntable rotates slowly.\n",
    "2. A fixed camera captures frames continuously to detect AprilTags.\n",
    "3. When the target tag is found, the turntable stops and the corresponding vial is aligned for interaction.\n",
    "4. The tag’s position is recalculated and converted into robot coordinates using a pre-calibrated transformation matrix.\n",
    "5. The robot arm grabs or places the vial from the side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f7699",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "To explore the demo with default files and settings, enable test mode below. If test mode is enabled, you can skip all parameter modifications in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f839cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Set `test = True` to use default files and parameters.\n",
    "test = True  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f6ed5",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "This notebook requires the following files and libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6069e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only install if we are running in colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install --upgrade gradio_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f5845",
   "metadata": {},
   "source": [
    "## Live Stream\n",
    "\n",
    "##### Watch the live demo here: https://www.youtube.com/playlist?list=PL8uZlc2CEpelrYXunUzUOMJC17wEhiP6Q\n",
    "\n",
    "This live stream shows the dedicated hardware in action, giving you a real-time view of what’s happening in the lab.\n",
    "You’ll see the AprilTags, turntable, and cobot working together to complete the workflow.\n",
    "As you run the notebook, each step is mirrored by real-world motion.\n",
    "\n",
    "> Tips: For a smoother experience, use Picture-in-Picture mode to keep the live stream visible while interacting with the notebook (right-click the video twice in Google Chrome or Microsoft Edge), or consider using split screen to view the notebook and stream side by side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1876e",
   "metadata": {},
   "source": [
    "## Static configuration\n",
    "\n",
    "Predefined system parameters, determined by the hardware setup and relative positioning.\n",
    "\n",
    "#### **⚠ Do not modify any of them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0390f708",
   "metadata": {},
   "source": [
    "\n",
    "### Tracking vials in turntable\n",
    "To track each specific vial and its current position, as well as whether a turntable position is occupied, we maintain two bidirectional mappings in the program:\n",
    "\n",
    "- Turntable ID ↔ AprilTag ID\n",
    "- Turntable ID ↔ Vial ID\n",
    "\n",
    "This allows the program to identify the corresponding AprilTag and vial for each position, so users can assign a specific vial to a specific slot.\n",
    "\n",
    "> Note:  You can refer to the livestream to visually confirm the mapping. The key in the vial_to_turntable dictionary is the vial ID, and the value is the turntable position ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ff37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static configuration\n",
    "# Only positions currently occupied by vials\n",
    "vial_to_turntable = {\n",
    "    'vial_1': 3, # vial_1 is currently placed at turntable position 3\n",
    "    'vial_2': 1,\n",
    "    'vial_3': 5,\n",
    "}\n",
    "\n",
    "# All turntable positions and their associated AprilTags\n",
    "turntable_to_tag = {\n",
    "    1: 'tag_01', # turntable position 1 is labeled by tag_01\n",
    "    2: 'tag_02',\n",
    "    3: 'tag_03',\n",
    "    4: 'tag_04',\n",
    "    5: 'tag_05',\n",
    "    6: 'tag_06',\n",
    "    7: 'tag_07',\n",
    "    8: 'tag_08',\n",
    "    \n",
    "}\n",
    "\n",
    "# Generate reverse mappings\n",
    "tag_to_turntable = {v: k for k, v in turntable_to_tag.items()}\n",
    "turntable_to_vial = {v: k for k, v in vial_to_turntable.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77554b10",
   "metadata": {},
   "source": [
    "### Camera Intrinsics\n",
    "\n",
    "Camera intrinsics obtained from camera calibration, including camera matrix and distortion coefficients. Used for AprilTag detection and pose estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31395062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load Camera Intrinsics (DO NOT MODIFY)\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "current_dir = os.path.dirname(__file__)  \n",
    "data_dir = os.path.join(current_dir, \"apriltag_hardware_demo\")\n",
    "camera_params_path = camera_params_path = os.path.join(data_dir, \"camera_params.npy\")\n",
    "camera_params = np.load(camera_params_path, allow_pickle=True).item()\n",
    "K = camera_params[\"camera_matrix\"]\n",
    "dist = camera_params[\"dist_coeff\"]\n",
    "\n",
    "camera_intrinsics = (K, dist)\n",
    "print(camera_intrinsics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1b1c9",
   "metadata": {},
   "source": [
    "\n",
    "### Hand-Eye Calibration Result\n",
    "\n",
    "Camera intrinsics obtained from camera calibration, including camera matrix and distortion coefficients. Used for predicting vial positions and performing grasping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load Hand-Eye Calibration (DO NOT MODIFY)\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "current_dir = os.path.dirname(__file__)\n",
    "data_dir = os.path.join(current_dir, \"apriltag_hardware_demo\")\n",
    "hand_eye_path = os.path.join(data_dir, \"hand_eye_calibration.npy\")\n",
    "\n",
    "hand_eye_params = np.load(hand_eye_path, allow_pickle=True).item()\n",
    "rotation_matrix = hand_eye_params[\"rotation_matrix\"]\n",
    "translation_vector = hand_eye_params[\"translation_vector\"]\n",
    "\n",
    "hand_eye_calibration = (rotation_matrix, translation_vector )\n",
    "print(hand_eye_calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c5679",
   "metadata": {},
   "source": [
    "### Default Positions\n",
    "\n",
    "- **Initial Home Pose**: Moves the cobot to all-zero joint angles as the home or reset position.\n",
    "- **Default Detection Pose**: Moves the cobot to a pre-defined joint configuration for AprilTag detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Default Detection Pose and Initial Home Pose Configuration (DO NOT MODIFY)\n",
    "\n",
    "INITIAL_HOME_POSE = {\n",
    "    \"angle0\": 0,\n",
    "    \"angle1\": 0,\n",
    "    \"angle2\": 0,\n",
    "    \"angle3\": 0,\n",
    "    \"angle4\": 0,\n",
    "    \"angle5\": 0,\n",
    "    \"movement_speed\": 50\n",
    "}\n",
    "\n",
    "DEFAULT_DETECTION_POSE = {\n",
    "    \"angle0\": 47,\n",
    "    \"angle1\": -65,\n",
    "    \"angle2\": -65,\n",
    "    \"angle3\": 132,\n",
    "    \"angle4\": 0,\n",
    "    \"angle5\": 132,\n",
    "    \"movement_speed\": 50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dff57c",
   "metadata": {},
   "source": [
    "### Grasp Offset\n",
    "\n",
    "Fixed offset from the AprilTag center to the vial position behind it, defined in the tag's coordinate frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Grasp Offset Configuration (DO NOT MODIFY)\n",
    "\n",
    "VIAL_OFFSET_IN_TAG = np.array([0.0, -0.015, 0.025])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcea399",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a11cb8",
   "metadata": {},
   "source": [
    "### `detect_apriltags`\n",
    "This is a helper function for detecting AprilTags.\n",
    "- When `only_ID=True`, it returns only the tag IDs, which is useful for identifying the correct AprilTag while the turntable is rotating.\n",
    "- When `only_ID=False`, it returns both the position and orientation of each tag, which is used by the cobot to place or pick up a vial after the turntable has stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768be88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pupil_apriltags import Detector\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_apriltags(image_path, camera_matrix, dist_coeffs, tag_size_meters, families=\"tagStandard41h12\", only_ID=True, show_image=False):\n",
    "    \"\"\"\n",
    "    AprilTag detection function with built-in undistortion.\n",
    "    \"\"\"\n",
    "    # Load grayscale image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image at {image_path}\")\n",
    "\n",
    "    # Undistort image first\n",
    "    undistorted_image = cv2.undistort(image, camera_matrix, dist_coeffs)\n",
    "\n",
    "    # Extract fx, fy, cx, cy for pupil_apriltags input\n",
    "    fx, fy = camera_matrix[0,0], camera_matrix[1,1]\n",
    "    cx, cy = camera_matrix[0,2], camera_matrix[1,2]\n",
    "    pupil_camera_params = [fx, fy, cx, cy]\n",
    "\n",
    "    # Initialize detector\n",
    "    at_detector = Detector(\n",
    "        families=families,\n",
    "        nthreads=1,\n",
    "        quad_decimate=1.0,\n",
    "        quad_sigma=0.0,\n",
    "        refine_edges=1,\n",
    "        decode_sharpening=0.25,\n",
    "        debug=0\n",
    "    )\n",
    "\n",
    "    detections = at_detector.detect(\n",
    "        undistorted_image,\n",
    "        estimate_tag_pose=True,\n",
    "        camera_params=pupil_camera_params,\n",
    "        tag_size=tag_size_meters\n",
    "    )\n",
    "\n",
    "    if only_ID:\n",
    "        return [det.tag_id for det in detections]\n",
    "\n",
    "    print(f\"Found {len(detections)} tags\")\n",
    "    for det in detections:\n",
    "        tag_id = det.tag_id\n",
    "        center = det.center\n",
    "        corners = det.corners.astype(int)\n",
    "\n",
    "        cv2.polylines(undistorted_image, [corners], True, 255, 2)\n",
    "        cx, cy = map(int, center)\n",
    "        cv2.circle(undistorted_image, (cx, cy), 4, 255, -1)\n",
    "        cv2.putText(undistorted_image, str(tag_id), (cx, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 255, 1)\n",
    "\n",
    "        print(f\"\\nTag ID: {tag_id}\")\n",
    "        print(f\"  Center: {center}\")\n",
    "        print(f\"  Corners: {det.corners}\")\n",
    "\n",
    "        if det.pose_t is not None:\n",
    "            t = det.pose_t.flatten()\n",
    "            print(f\"  Translation (x, y, z): {t}\")\n",
    "\n",
    "            distance_cm = np.linalg.norm(t) * 100\n",
    "            print(f\"  Distance from camera: {distance_cm:.2f} centimeters\")\n",
    "\n",
    "            euler = R.from_matrix(det.pose_R).as_euler('xyz', degrees=True)\n",
    "            roll, pitch, yaw = euler\n",
    "            print(f\"  Roll: {roll:.2f}°, Pitch: {pitch:.2f}°, Yaw: {yaw:.2f}°\")\n",
    "        else:\n",
    "            print(\"  No pose estimated (pose_t is None)\")\n",
    "\n",
    "    if show_image:\n",
    "        plt.imshow(undistorted_image, cmap='gray')\n",
    "        plt.title(\"AprilTag Detection (Undistorted)\")\n",
    "        plt.show()\n",
    "\n",
    "    return detections\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93892a",
   "metadata": {},
   "source": [
    "### `display_image` and `display_result`\n",
    "\n",
    "- `display_image(image_path)`: Displays the image at the given path (used to check camera view or results).\n",
    "- `display_result(result)`: Parses and prints server response, shows queue status and detection results, and displays image if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4203ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "def display_image(image_path):\n",
    "\ttry:\n",
    "\t\timg = Image.open(image_path).convert(\"RGB\")\n",
    "\t\tplt.imshow(img)\n",
    "\t\tplt.title(\"Cobot view\")\n",
    "\t\tplt.show()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"An error occurred: {e}\")\n",
    "\n",
    "def display_result(result):\n",
    "\tqueue_status_str = result[-1].replace('\\n', ' ')\n",
    "\tprint(f\"queue status: {queue_status_str}\")\n",
    "\tprint(f\"response json: {None if result[0] is None else json.loads(result[0])}\")\n",
    "\tif len(result) == 3:\n",
    "\t\tif result[1] is None:\n",
    "\t\t\treturn\n",
    "\t\tdisplay_image(result[1]['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08c9dbb",
   "metadata": {},
   "source": [
    "### `decompose_transform` and `convert`\n",
    "\n",
    "- `decompose_transform(matrix)`: Decomposes a 4x4 homogeneous transformation matrix into translation and Euler angles (rx, ry, rz). Used to extract position and orientation from computed poses.\n",
    "- `convert(x, y, z, rx, ry, rz, x1, y1, z1, rx1, ry1, rz1, hand_eye_calibration)`: Computes the object pose in the robot base frame by chaining the robot's end-effector pose, the detected object pose from camera, and the hand-eye calibration result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec36f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "import numpy as np\n",
    "\n",
    "def decompose_transform(matrix):\n",
    "    \"\"\"\n",
    "    Decompose homogeneous matrix to translation + euler angles (xyz)\n",
    "    \"\"\"\n",
    "    translation = matrix[:3, 3]\n",
    "    rotation = matrix[:3, :3]\n",
    "\n",
    "    # Convert rotation matrix to euler angles\n",
    "    sy = np.sqrt(rotation[0, 0] ** 2 + rotation[1, 0] ** 2)\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if not singular:\n",
    "        rx = np.arctan2(rotation[2, 1], rotation[2, 2])\n",
    "        ry = np.arctan2(-rotation[2, 0], sy)\n",
    "        rz = np.arctan2(rotation[1, 0], rotation[0, 0])\n",
    "    else:\n",
    "        rx = np.arctan2(-rotation[1, 2], rotation[1, 1])\n",
    "        ry = np.arctan2(-rotation[2, 0], sy)\n",
    "        rz = 0\n",
    "\n",
    "    return translation, rx, ry, rz\n",
    "\n",
    "\n",
    "def convert(x, y, z, rx, ry, rz, x1, y1, z1, rx1, ry1, rz1, hand_eye_calibration):\n",
    "    \"\"\"\n",
    "    Compute object pose in robot base frame using hand-eye calibration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack hand-eye calibration\n",
    "    rotation_matrix, translation_vector = hand_eye_calibration\n",
    "\n",
    "    # Build T_camera_to_end_effector\n",
    "    T_camera_to_end_effector = np.eye(4)\n",
    "    T_camera_to_end_effector[:3, :3] = rotation_matrix\n",
    "    T_camera_to_end_effector[:3, 3] = translation_vector\n",
    "\n",
    "    # Build T_end_to_base_effector (robot's current pose)\n",
    "    position = np.array([x, y, z])\n",
    "    orientation = R.from_euler('xyz', [rx, ry, rz], degrees=False).as_matrix()\n",
    "\n",
    "    T_end_to_base_effector = np.eye(4)\n",
    "    T_end_to_base_effector[:3, :3] = orientation\n",
    "    T_end_to_base_effector[:3, 3] = position\n",
    "\n",
    "    # Build T_object_to_camera_effector (object pose seen by camera)\n",
    "    position2 = np.array([x1, y1, z1])\n",
    "    orientation2 = R.from_euler('xyz', [rx1, ry1, rz1], degrees=False).as_matrix()\n",
    "\n",
    "    T_object_to_camera_effector = np.eye(4)\n",
    "    T_object_to_camera_effector[:3, :3] = orientation2\n",
    "    T_object_to_camera_effector[:3, 3] = position2\n",
    "\n",
    "    # Chain the transforms:\n",
    "    obj_in_end_effector = T_camera_to_end_effector @ T_object_to_camera_effector\n",
    "    obj_in_base = T_end_to_base_effector @ obj_in_end_effector\n",
    "\n",
    "    # Decompose result\n",
    "    result = decompose_transform(obj_in_base)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d13157",
   "metadata": {},
   "source": [
    "### `move_to_pose`\n",
    "\n",
    "- Used to move the robot directly to the computed grasping position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ec6e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_pose(result_pose, movement_speed=50):\n",
    "    position, rx, ry, rz = result_pose\n",
    "    x_mm, y_mm, z_mm = position * 1000\n",
    "    roll_deg = np.degrees(rx)\n",
    "    pitch_deg = np.degrees(ry)\n",
    "    yaw_deg = np.degrees(rz)\n",
    "\n",
    "    result = client.predict(\n",
    "        user_id=USER_ID,\n",
    "        x=x_mm,\n",
    "        y=y_mm,\n",
    "        z=z_mm,\n",
    "        roll=roll_deg,\n",
    "        pitch=pitch_deg,\n",
    "        yaw=yaw_deg,\n",
    "        movement_speed=movement_speed,\n",
    "        api_name=\"/control_coords\"\n",
    "    )\n",
    "    display_result(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef46b5",
   "metadata": {},
   "source": [
    "### `move_relative`\n",
    "\n",
    "- Move the cobot by a relative offset (in mm) along X, Y, Z directions based on current pose.\n",
    "- The orientation remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_relative(dx=0.0, dy=0.0, dz=0.0, movement_speed=50):\n",
    "    \"\"\"\n",
    "    Move the cobot relative to its current position along X, Y, Z axes (in mm).\n",
    "    \"\"\"\n",
    "    # Get current coordinates\n",
    "    result = client.predict(user_id=USER_ID, api_name=\"/query_coords\")\n",
    "    coords = json.loads(result[0])[\"coords\"]\n",
    "    \n",
    "    # Extract current position and orientation\n",
    "    x_curr, y_curr, z_curr = coords[0], coords[1], coords[2]\n",
    "    roll, pitch, yaw = coords[3], coords[4], coords[5]\n",
    "    \n",
    "    # Compute new position\n",
    "    x_new = x_curr + dx\n",
    "    y_new = y_curr + dy\n",
    "    z_new = z_curr + dz\n",
    "\n",
    "    # Send new command\n",
    "    result = client.predict(\n",
    "        user_id=USER_ID,\n",
    "        x=x_new,\n",
    "        y=y_new,\n",
    "        z=z_new,\n",
    "        roll=roll,\n",
    "        pitch=pitch,\n",
    "        yaw=yaw,\n",
    "        movement_speed=movement_speed,\n",
    "        api_name=\"/control_coords\"\n",
    "    )\n",
    "    display_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7c939",
   "metadata": {},
   "source": [
    "## Connecting to the Cobot\n",
    "\n",
    "### Initialize Gradio Client\n",
    "\n",
    "- Create a unique `USER_ID` for the current session.\n",
    "- Connect to the remote Gradio server (`cobot280pi-gradio-g9sv`) using Hugging Face token.\n",
    "- `client.view_api()` lists available API endpoints for interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a990b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio_client import Client\n",
    "import uuid\n",
    "import getpass  \n",
    "\n",
    "USER_ID = str(uuid.uuid4())\n",
    "print(f\"Your user id: {USER_ID}\")\n",
    "\n",
    "hf_token = getpass.getpass(\"Enter your Hugging Face Token:\")\n",
    "\n",
    "client = Client(\n",
    "    \"AccelerationConsortium/cobot280pi-gradio-g9sv\",\n",
    "    hf_token=hf_token\n",
    ")\n",
    "\n",
    "client.view_api()\n",
    "\n",
    "result = client.predict(\n",
    "    user_id=USER_ID,\n",
    "    api_name=\"/enter_queue\"\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f9f090",
   "metadata": {},
   "source": [
    "### Enter Queue\n",
    "\n",
    "- Call `/enter_queue` API to join the task queue.\n",
    "- The server assigns a queue position and returns status info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.predict(\n",
    "    user_id=USER_ID,\n",
    "    api_name=\"/enter_queue\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d5e1c",
   "metadata": {},
   "source": [
    "## Cobot Vial Pick-and-Place Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba1fc2",
   "metadata": {},
   "source": [
    "First, move the cobot to the default place to detect apriltag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d64a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.predict(\n",
    "    user_id=USER_ID,\n",
    "    **DEFAULT_DETECTION_POSE, \n",
    "    api_name=\"/control_angles\"\n",
    ")\n",
    "display_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339f776",
   "metadata": {},
   "source": [
    "Then detect the AprilTag. If no tag is detected, retry several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.predict(\n",
    "\tuser_id=USER_ID,\n",
    "\tapi_name=\"/query_camera\"\n",
    ")\n",
    "print(result)\n",
    "display_result(result)\n",
    "image_path = result[1][\"value\"]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "camera_params = np.load(\"camera_params.npy\", allow_pickle=True).item()\n",
    "\n",
    "detections = detect_apriltags(\n",
    "    image_path,\n",
    "    camera_matrix=camera_params['camera_matrix'],\n",
    "    dist_coeffs=camera_params['dist_coeff'],\n",
    "    tag_size_meters=0.013,\n",
    "    families=\"tagStandard52h13\",\n",
    "    only_ID=False\n",
    ")\n",
    "\n",
    "det = detections[0]\n",
    "\n",
    "pose_t = det.pose_t.flatten()\n",
    "pose_R = det.pose_R\n",
    "\n",
    "# Transform offset to camera frame\n",
    "offset_in_camera = pose_R @ VIAL_OFFSET_IN_TAG\n",
    "\n",
    "# Apply offset\n",
    "corrected_pose_t = pose_t + offset_in_camera\n",
    "\n",
    "x1, y1, z1 = corrected_pose_t\n",
    "euler = R.from_matrix(pose_R).as_euler('xyz', degrees=False)\n",
    "rx1, ry1, rz1 = euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50223f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement turntable logic to rotate the turntable to the target position and grasp the specific vial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f750255",
   "metadata": {},
   "source": [
    "Query coordinate result from the cobot, then compute object pose in the robot base frame using end-effector pose, AprilTag detection, and hand-eye calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.predict(\n",
    "\tuser_id=USER_ID,\n",
    "\tapi_name=\"/query_coords\"\n",
    ")\n",
    "display_result(result)\n",
    "coords = json.loads(result[0])[\"coords\"]\n",
    "\n",
    "result_pose = convert(\n",
    "    coords[0], coords[1], coords[2], coords[3], coords[4], coords[5], \n",
    "    x1, y1, z1, rx1, ry1, rz1,                                         \n",
    "    hand_eye_calibration                                                \n",
    ")\n",
    "\n",
    "position, rx, ry, rz = result_pose\n",
    "print(\"Target pose in robot base frame:\")\n",
    "print(\"Position:\", position)\n",
    "print(\"Rotation (Euler angles):\", rx, ry, rz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb534b4",
   "metadata": {},
   "source": [
    "Open gripper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.predict(\n",
    "\tuser_id=USER_ID,\n",
    "\tgripper_value = 0,\n",
    "\tmovement_speed = 100,\n",
    "\tapi_name=\"/control_gripper\"\n",
    ")\n",
    "display_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abf3dc",
   "metadata": {},
   "source": [
    "Move cobot to target position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "move_to_pose(result_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fc5f68",
   "metadata": {},
   "source": [
    "Close gripper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd48193",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.predict(\n",
    "\tuser_id=USER_ID,\n",
    "\tgripper_value = 100,\n",
    "\tmovement_speed = 100,\n",
    "\tapi_name=\"/control_gripper\"\n",
    ")\n",
    "display_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0734c9e",
   "metadata": {},
   "source": [
    "Lift the vial after grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f58a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_relative(dz=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280bd1da",
   "metadata": {},
   "source": [
    "Move back to default place to detect apriltag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.predict(\n",
    "    user_id=USER_ID,\n",
    "    **DEFAULT_DETECTION_POSE, \n",
    "    api_name=\"/control_angles\"\n",
    ")\n",
    "display_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e8bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement turntable logic to rotate the turntable to the target position and put the specific vial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd861ba",
   "metadata": {},
   "source": [
    "Detect the AprilTag at the target slot. Retry if no tag is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1956e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.predict(\n",
    "\tuser_id=USER_ID,\n",
    "\tapi_name=\"/query_camera\"\n",
    ")\n",
    "print(result)\n",
    "display_result(result)\n",
    "image_path = result[1][\"value\"]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "camera_params = np.load(\"camera_params.npy\", allow_pickle=True).item()\n",
    "\n",
    "detections = detect_apriltags(\n",
    "    image_path,\n",
    "    camera_matrix=camera_params['camera_matrix'],\n",
    "    dist_coeffs=camera_params['dist_coeff'],\n",
    "    tag_size_meters=0.013,\n",
    "    families=\"tagStandard52h13\",\n",
    "    only_ID=False\n",
    ")\n",
    "\n",
    "det = detections[0]\n",
    "\n",
    "pose_t = det.pose_t.flatten()\n",
    "pose_R = det.pose_R\n",
    "\n",
    "# Transform offset to camera frame\n",
    "offset_in_camera = pose_R @ VIAL_OFFSET_IN_TAG\n",
    "\n",
    "# Apply offset\n",
    "corrected_pose_t = pose_t + offset_in_camera\n",
    "\n",
    "x1, y1, z1 = corrected_pose_t\n",
    "euler = R.from_matrix(pose_R).as_euler('xyz', degrees=False)\n",
    "rx1, ry1, rz1 = euler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2976ebe",
   "metadata": {},
   "source": [
    "Query coordinate result from the cobot, then compute object pose in the robot base frame using end-effector pose, AprilTag detection, and hand-eye calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.predict(\n",
    "\tuser_id=USER_ID,\n",
    "\tapi_name=\"/query_coords\"\n",
    ")\n",
    "display_result(result)\n",
    "coords = json.loads(result[0])[\"coords\"]\n",
    "\n",
    "result_pose = convert(\n",
    "    coords[0], coords[1], coords[2], coords[3], coords[4], coords[5], \n",
    "    x1, y1, z1, rx1, ry1, rz1,                                         \n",
    "    hand_eye_calibration                                                \n",
    ")\n",
    "\n",
    "position, rx, ry, rz = result_pose\n",
    "print(\"Target pose in robot base frame:\")\n",
    "print(\"Position:\", position)\n",
    "print(\"Rotation (Euler angles):\", rx, ry, rz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c28fd",
   "metadata": {},
   "source": [
    "Move down to place the vial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_relative(dz=-30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26455716",
   "metadata": {},
   "source": [
    "Open gripper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58897b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.predict(\n",
    "\tuser_id=USER_ID,\n",
    "\tgripper_value = 100,\n",
    "\tmovement_speed = 100,\n",
    "\tapi_name=\"/control_gripper\"\n",
    ")\n",
    "display_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4fad89",
   "metadata": {},
   "source": [
    "Finally, move the cobot to the home palce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a455c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.predict(\n",
    "    user_id=USER_ID,\n",
    "    **DEFAULT_DETECTION_POSE, \n",
    "    api_name=\"/control_angles\"\n",
    ")\n",
    "display_result(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
